# Keras编程

## 1. Sequential\_Functional\_Model

### 1.1 情感分类

#### 1.1.1 传统模型

即传统的机器学习方法。本项目重点在于用Keras实现深度学习模型，传统机器学习方法在之前的文本分类项目中有所体现。

#### 1.1.2 深度学习模型

实现了三个模型，即

* one\_hot：以字为单位，不分词，将每个句子截断为200字（不够则补空字符串）。然后将句子以“字\_one\_hot”的矩阵形式输入到LSTM模型中进行学习分类。
* one\_embedding：以字为单位，不分词，将每个句子截断为200字（不够则补空字符串）。然后将句子以“字\_字向量(embedding)”的矩阵形式输入到LSTM模型中进行学习分类。
* word\_embedding：以词为单位，分词，将每个句子截断为100词（不够则补空字符串）。然后将句子以“词\_词向量(embedding)”的矩阵形式输入到LSTM模型中进行学习分类。

具体思想是

one\_hot：

* one\_hot模型诟病的原因，除了维度灾难之外，还有一个就是“语义鸿沟”，也就说任意两个词之间没有任何相关性（不管用欧式距离还是余弦相似度，任意两个词的计算结果是一样的）。**虽然这一点假设用在词语中不成立，可是用在中文的“字”上面，感觉很合理。**汉字单独成词的例子不多，大多数是二字词，也就是说，任意两个字之间没有任何相关性，这个假设在汉字的“字”的层面上，是近似成立的！而后面我们用了LSTM，LSTM本身具有整合邻近数据的功能，因此，它暗含了将字整合为词的过程。
* one hot模型还有一个非常重要的特点是它没有任何信息损失，从one hot的编码结果中，我们反过来解码出原来那句话是哪些字词组成的。然而，我无法从一个词向量中确定原来的词是什么。这些观点都表明，在很多情况下，one hot模型都是很有价值的。

词向量：

* 词向量相当于做了一个假设：每个词具有比较确定的意思。这个假设在词语层面也是近似成立的，毕竟一词多义的词语相对来说也不多。正因为如此，我们才可以将词放到一个较低维度的实数空间里，用一个实数向量来表示一个词语，并且用它们之间的距离或者余弦相似度来表示词语之间的相似度。**这也是词向量能够解决“一义多词”而没法解决“一词多义”的原因。**

从这样看来，上面三个模型中，只有one\_hot和word\_embedding才是理论上说得过去的，而one\_embedding则看上去变得不伦不类了，因为字似乎不能说具有比较确定的意思。

但我们测试one\_embedding效果也还不错。**这可能是因为二元分类问题本身是一个很粗糙的分类（0或1），如果更多元的分类，可能one\_embedding的方式效果就降下来了。**不过，我也没有进行更多的测试了，大家可以选择多元分类的文本继续测试。

以上想法均为主观想法。

### 1.2 阅读理解机器人

采用RNN实现。

参考：

* Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush, "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", http://arxiv.org/abs/1502.05698
* Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, "End-To-End Memory Networks", http://arxiv.org/abs/1503.08895

数据集：

https://s3.amazonaws.com/text-datasets/babi\_tasks\_1-20\_v1-2.tar.gz

### 1.3 CRF层

Keras实现CRF层。

## 2. 预训练模型Application

### 2.1 VGG16 

采用VGG16模型进行预测功能，导入预训练的参数；增加与任务相关的全连接层。保持VGG16的第五层卷积和全连接层进行参数微调；其他卷积层采用预训练的参数，参数设置为不可训练。
